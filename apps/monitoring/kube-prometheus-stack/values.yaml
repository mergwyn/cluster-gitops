---
env:
  TZ: Europe/London

prometheus-node-exporter:
  commonLabels:
    jobLabel: node-exporter
  resources:
    requests:
      cpu: 15m
      memory: 105M
    limits:
      # cpu: 15m
      memory: 105M

crds:
  enabled: true
  upgradeJob:
    enabled: true
    eorceConflicts: true

alertmanager:
  config:
    global:
      smtp_require_tls: false
    route:
      receiver: 'email-k8s-admin'
      routes:
        - receiver: 'null'
          matchers:
            - alertname =~ "InfoInhibitor|Watchdog"
        - receiver: 'email-k8s-admin'
    receivers:
      - name: 'null'
      - name: 'email-k8s-admin'
        email_configs:
          - to: gary@theclarkhome.com
            from: email-k8s-admin@theclarkhome.com
            smarthost: mail.default:587
  spec:
    resources:
      requests:
        cpu: 15m
        memory: 105M
      limits:
        # cpu: 15m
        memory: 105M

  ingress:
    enabled: true
    ingressClass: traefik
    annotations:
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
    hosts:
      - "alertmanager.theclarkhome.com"
    tls:
      - hosts:
          - "*.theclarkhome.com"

#  storage:
#    volumeClaimTemplate:
#      spec:
#        storageClassName: openebs-hostpath
#        accessModes: ["ReadWriteOnce"]
#        resources:
#          requests:
#            storage: 50Gi

prometheus:
  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false
    # podMonitorSelectorNilUsesHelmValues: false
    additionalScrapeConfigs:
      - job_name: minio-job-v2
        # yamllint disable-line rule:line-length
        bearer_token: eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwcm9tZXRoZXVzIiwic3ViIjoibXltaW5pb2FkbWluIiwiZXhwIjo0ODY1MjQ0NzEyfQ.21LxcoJnfop1f4eqy2NedKaeTaDw7RhiWr_ZnjoUxo11pcoIlSNDwpW93dU2Gi4XMi-yv8piDm4FU0MAObPJhg
        metrics_path: /minio/v2/metrics/cluster
        scheme: http
        static_configs:
          - targets: ['foxtrot.theclarkhome.com:9000']

      - job_name: blikvm-v1
        metrics_path: '/api/export/prometheus/metrics'
        basic_auth:
          username: admin
          password: admin
        scheme: https
        static_configs:
          - targets: ['blikvm.theclarkhome.com']
        tls_config:
          insecure_skip_verify: true  # For self-signed certificate

    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: openebs-zfspv
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    resources:
      requests:
        cpu: 180m
        memory: 512M
      limits:
        # cpu: 15m
        memory: 5G

  ingress:
    enabled: true
    ingressClass: traefik
    annotations:
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
    hosts:
      - "prometheus.theclarkhome.com"
    tls:
      - hosts:
          - "*.theclarkhome.com"

grafana:
  adminPassword: ""
  admin:
    ## Name of the secret. Can be templated.
    existingSecret: grafana-admin
    userKey: admin-user
    passwordKey: admin-password
  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki.monitoring.svc.cluster.local:3100
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          # folder: 'Default'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/standard
  dashboards:
    default:
      # hwmon:
      #   gnetId: 12950
      #   datasource: Prometheus
      # promtail:
      #   gnetId: 15141
      #   datasource: Loki
      # minio:
      #   gnetId: 13502
      #   datasource: Prometheus
      # minio-server:
      #   name: "MinIO server"
      # yamllint disable-line rule:line-length
      #   url: "https://raw.githubusercontent.com/minio/minio/master/docs/metrics/prometheus/grafana/minio-dashboard.json"
      #   datasource: Prometheus
      # minio-bucket:
      #   name: "MinIO bucket"
      # yamllint disable-line rule:line-length
      #   url: "https://raw.githubusercontent.com/minio/minio/master/docs/metrics/prometheus/grafana/bucket/minio-bucket.json"
      #   datasource: Prometheus
      # minio-node:
      #   name: "MinIO node"
      # yamllint disable-line rule:line-length
      #   url: "https://raw.githubusercontent.com/minio/minio/master/docs/metrics/prometheus/grafana/node/minio-node.json"
      #   datasource: Prometheus
      zfspv:
        # yamllint disable-line rule:line-length
        url: "https://raw.githubusercontent.com/openebs/zfs-localpv/develop/deploy/sample/grafana-dashboard.json"
        datasource: Prometheus
  ingress:
    enabled: true
    ingressClass: traefik
    annotations:
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
    hosts:
      - "grafana.theclarkhome.com"
    tls:
      - hosts:
          - "*.theclarkhome.com"
  persistence:
    enabled: true
    type: pvc
    storageClassName: openebs-zfspv
    size: 10Gi
  resources:
    requests:
      cpu: 15m
      memory: 418M
    limits:
      # cpu: 15m
      memory: 3G

resources:
  limits:
    # cpu: 400m
    memory: 512Mi
  requests:
    cpu: 200m
    memory: 512Mi

kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false
kubeProxy:
  enabled: false

prometheusOperator:
  resources:
    requests:
      cpu: 15m
      memory: 105M
    limits:
      # cpu: 15m
      memory: 105M

kube-state-metrics:
  resources:
    requests:
      cpu: 15m
      memory: 279M
    limits:
      # cpu: 15m
      memory: 279M

defaultRules:
  rules:
    kubeProxy: false
    # yamllint disable-line rule:line-length
    KubeClientCertificateExpiration: false  # See https://github.com/prometheus-community/helm-charts/issues/3441

# yamllint disable rule:line-length
additionalPrometheusRulesMap:
  additional-rules:
    groups:
      - name: oomkills
        rules:
          - alert: HostOomKillDetected
            expr: (increase(node_vmstat_oom_kill[1m]) > 0)
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Host OOM kill detected (instance {{ $labels.instance }})
              description: |-
                "OOM kill detected\n VALUE = {{ $value }}\n LABELS = {{ $labels }}"
          - alert: KubernetesContainerOomKiller
            expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes Container oom killer (instance {{ $labels.instance }})
              description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

#          - alert: OomKillEvents
#            expr: (sum by(namespace, pod) (kube_pod_container_status_last_terminated_reason{reason=“OOMKilled”}) > 0)
#            for: 30s
#            labels:
#              issue: The pod {{$labels.pod}} in {{$labels.namespace}} was recently OOMkilled.
#              severity: critical
#            annotations:
#              description: |-
#                "The pod {{$labels.pod}} in {{$labels.namespace}} was recently OOMkilled."
#              summary: Pod was OOMKilled
