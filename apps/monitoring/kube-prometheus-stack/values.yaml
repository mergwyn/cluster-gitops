---
env:
  TZ: Europe/London

prometheus-node-exporter:
  commonLabels:
    jobLabel: node-exporter
  resources:
    requests:
      cpu: 15m
      memory: 105M
    limits:
      memory: 256M

crds:
  enabled: true
  upgradeJob:
    enabled: true
    #forceConflicts: true

alertmanager:
  config:
    global:
      smtp_require_tls: false
    route:
      receiver: 'email-k8s-admin'
      routes:
        - receiver: 'null'
          matchers:
            - alertname =~ "InfoInhibitor|Watchdog"
        - receiver: 'email-k8s-admin'
    receivers:
      - name: 'null'
      - name: 'email-k8s-admin'
        email_configs:
          - to: gary@theclarkhome.com
            from: email-k8s-admin@theclarkhome.com
            smarthost: mail.system:587
  spec:
    resources:
      requests:
        cpu: 15m
        memory: 105M
      limits:
        # cpu: 15m
        memory: 105M


  route:
    main:
      enabled: true
      hostnames:
        - "alertmanager.{{.Values.global.domain}}"
      parentRefs:
        - name: traefik-internal
          namespace: network

#  storage:
#    volumeClaimTemplate:
#      spec:
#        storageClassName: openebs-hostpath
#        accessModes: ["ReadWriteOnce"]
#        resources:
#          requests:
#            storage: 50Gi

prometheus:
  prometheusSpec:

#     enableOTLPReceiver: true
#     global:
#       evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
#     otlp:
#       # Recommended attributes to be promoted to labels.
#       promote_resource_attributes:
#         - service.instance.id
#         - service.name
#         - service.namespace
#         - cloud.availability_zone
#         - cloud.region
#         - container.name
#         - deployment.environment.name
#         - k8s.cluster.name
#         - k8s.container.name
#         - k8s.cronjob.name
#         - k8s.daemonset.name
#         - k8s.deployment.name
#         - k8s.job.name
#         - k8s.namespace.name
#         - k8s.pod.name
#         - k8s.replicaset.name
#         - k8s.statefulset.name
#       # Ingest OTLP data keeping all characters in metric/label names.
#       # translation_strategy: NoUTF8EscapingWithSuffixes
#     storage:
#       # OTLP is a push-based protocol, Out of order samples is a common scenario.
#       tsdb:
#         out_of_order_time_window: 30m  

    serviceMonitorSelectorNilUsesHelmValues: false
    # podMonitorSelectorNilUsesHelmValues: false
    additionalScrapeConfigs:
      - job_name: minio-job-v2
        # yamllint disable-line rule:line-length
        bearer_token: eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwcm9tZXRoZXVzIiwic3ViIjoibXltaW5pb2FkbWluIiwiZXhwIjo0ODY1MjQ0NzEyfQ.21LxcoJnfop1f4eqy2NedKaeTaDw7RhiWr_ZnjoUxo11pcoIlSNDwpW93dU2Gi4XMi-yv8piDm4FU0MAObPJhg
        metrics_path: /minio/v2/metrics/cluster
        scheme: http
        static_configs:
          - targets: ['foxtrot.theclarkhome.com:9000']

#       - job_name: rustfs-job-v1
#         # yamllint disable-line rule:line-length
#         bearer_token: eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwcm9tZXRoZXVzIiwic3ViIjoibXltaW5pb2FkbWluIiwiZXhwIjo0ODY1MjQ0NzEyfQ.21LxcoJnfop1f4eqy2NedKaeTaDw7RhiWr_ZnjoUxo11pcoIlSNDwpW93dU2Gi4XMi-yv8piDm4FU0MAObPJhg
#         metrics_path: /rustfs/metrics
#         basic_auth:
#           username: rustadmin
#           password: rustadmin
#         scheme: http
#         static_configs:
#           - targets: ['foxtrot.theclarkhome.com:9090']

      - job_name: blikvm-v1
        metrics_path: '/api/export/prometheus/metrics'
        basic_auth:
          username: admin
          password: admin
        scheme: https
        static_configs:
          - targets: ['blikvm.theclarkhome.com']
        tls_config:
          insecure_skip_verify: true  # For self-signed certificate

    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: openebs-zfspv
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    resources:
      requests:
        cpu: 180m
        memory: 512M
      limits:
        # cpu: 15m
        memory: 5G

  route:
    main:
      enabled: true
      hostnames:
        - "prometheus.{{.Values.global.domain}}"
      parentRefs:
        - name: traefik-internal
          namespace: network

grafana:
  adminPassword: ""
  admin:
    ## Name of the secret. Can be templated.
    existingSecret: grafana-admin
    userKey: admin-user
    passwordKey: admin-password
  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki.monitoring.svc.cluster.local:3100
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          # folder: 'Default'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/standard
  dashboards:
    default:
      # hwmon:
      #   gnetId: 12950
      #   datasource: Prometheus
      # promtail:
      #   gnetId: 15141
      #   datasource: Loki
      # minio:
      #   gnetId: 13502
      #   datasource: Prometheus
      # minio-server:
      #   name: "MinIO server"
      # yamllint disable-line rule:line-length
      #   url: "https://raw.githubusercontent.com/minio/minio/master/docs/metrics/prometheus/grafana/minio-dashboard.json"
      #   datasource: Prometheus
      # minio-bucket:
      #   name: "MinIO bucket"
      # yamllint disable-line rule:line-length
      #   url: "https://raw.githubusercontent.com/minio/minio/master/docs/metrics/prometheus/grafana/bucket/minio-bucket.json"
      #   datasource: Prometheus
      # minio-node:
      #   name: "MinIO node"
      # yamllint disable-line rule:line-length
      #   url: "https://raw.githubusercontent.com/minio/minio/master/docs/metrics/prometheus/grafana/node/minio-node.json"
      #   datasource: Prometheus
      zfspv:
        # yamllint disable-line rule:line-length
        url: "https://raw.githubusercontent.com/openebs/zfs-localpv/develop/deploy/sample/grafana-dashboard.json"
        datasource: Prometheus
  route:
    main:
      enabled: true
      hostnames:
        - "grafana.{{.Values.global.domain}}"
      parentRefs:
        - name: traefik-internal
          namespace: network
  persistence:
    enabled: true
    type: pvc
    storageClassName: openebs-zfspv
    size: 10Gi
    annotations:
      helm.sh/resource-policy: "keep"
  resources:
    requests:
      cpu: 15m
      memory: 418M
    limits:
      # cpu: 15m
      memory: 3G

resources:
  limits:
    # cpu: 400m
    memory: 512Mi
  requests:
    cpu: 200m
    memory: 512Mi

kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false
kubeProxy:
  enabled: false

prometheusOperator:
  resources:
    requests:
      cpu: 15m
      memory: 105M
    limits:
      # cpu: 15m
      memory: 105M

kube-state-metrics:
  resources:
    requests:
      cpu: 15m
      memory: 279M
    limits:
      # cpu: 15m
      memory: 279M

defaultRules:
  rules:
    kubeProxy: false
    # See https://github.com/prometheus-community/helm-charts/issues/3441
    KubeClientCertificateExpiration: false

# yamllint disable rule:line-length
additionalPrometheusRulesMap:
  additional-rules:
    groups:
      - name: oomkills
        rules:
          - alert: HostOomKillDetected
            expr: (increase(node_vmstat_oom_kill[1m]) > 0)
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Host OOM kill detected (instance {{ $labels.instance }})
              description: |-
                "OOM kill detected\n VALUE = {{ $value }}\n LABELS = {{ $labels }}"
          - alert: KubernetesContainerOomKiller
            expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes Container oom killer (instance {{ $labels.instance }})
              description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

#          - alert: OomKillEvents
#            expr: (sum by(namespace, pod) (kube_pod_container_status_last_terminated_reason{reason=“OOMKilled”}) > 0)
#            for: 30s
#            labels:
#              issue: The pod {{$labels.pod}} in {{$labels.namespace}} was recently OOMkilled.
#              severity: critical
#            annotations:
#              description: |-
#                "The pod {{$labels.pod}} in {{$labels.namespace}} was recently OOMkilled."
#              summary: Pod was OOMKilled
